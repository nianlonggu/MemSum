{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNnt5iE3SWhcUyMT6RH+9EX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nianlonggu/MemSum/blob/main/Data_processing_training_and_testing_for_MemSum.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparation"
      ],
      "metadata": {
        "id": "gQ1RHhs_EoCY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clone the Repo"
      ],
      "metadata": {
        "id": "-pDHG2nTEyMk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0ZhUm14DVMU",
        "outputId": "5c71a065-1e17-4650-be69-4463cbf8a962"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'MemSum'...\n",
            "remote: Enumerating objects: 203, done.\u001b[K\n",
            "remote: Counting objects: 100% (105/105), done.\u001b[K\n",
            "remote: Compressing objects: 100% (91/91), done.\u001b[K\n",
            "remote: Total 203 (delta 39), reused 54 (delta 9), pack-reused 98\u001b[K\n",
            "Receiving objects: 100% (203/203), 81.90 MiB | 16.89 MiB/s, done.\n",
            "Resolving deltas: 100% (58/58), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/nianlonggu/MemSum.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Change the working directory to the main folder of MemSum"
      ],
      "metadata": {
        "id": "HqonYcpJErsS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.chdir(\"MemSum\")"
      ],
      "metadata": {
        "id": "h3g3D88DEFI3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install packages\n",
        "\n",
        "Note: Because colab has preinstalled torch, so we don't need to install pytorch again\n",
        "\n",
        "We tested on torch version>=1.11.0."
      ],
      "metadata": {
        "id": "MAlzv2UCE5vS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt -q"
      ],
      "metadata": {
        "id": "Fw1sOX0AEUzV"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "Ky4m3pQNSAVB",
        "outputId": "fa7bd4f8-bf56-4dc9-a05a-b361b9ba1a8a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.12.1+cu113'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing Custom data"
      ],
      "metadata": {
        "id": "9IlyFZtEFW5V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Suppose that you have already splitted the training / validation and  test set:\n",
        "\n",
        "The training data is now stored in a .jsonl file that contains a list of json info, one line for one training instance. Each json (or dictonary) contains two keys: \n",
        "\n",
        "1. \"text\": the value for which is a python list of sentences, this represents the document you want to summarize;\n",
        "2. \"summary\": the value is also a list of sentences. If represent the ground-truth summary. Because the summary can contain multiple sentences, so we store them as a list.\n",
        "\n",
        "The same for the validation file and the testing file. \n"
      ],
      "metadata": {
        "id": "I2PqJpw3FcTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "train_corpus = [ json.loads(line) for line in open(\"data/custom_data/train_CUSTOM_raw.jsonl\") ]\n",
        "\n",
        "## as an example, we have 100 instances for training\n",
        "print(len(train_corpus))\n",
        "print(train_corpus[0].keys())\n",
        "print(train_corpus[0][\"text\"][:3])\n",
        "print(train_corpus[0][\"summary\"][:3])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Q0tTRp8Golc",
        "outputId": "6c7d6ec7-a8fb-4759-dbdd-c9bf1e3a5795"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n",
            "dict_keys(['text', 'summary'])\n",
            "['a recent systematic analysis showed that in 2011 , 314 ( 296 - 331 ) million children younger than 5 years were mildly , moderately or severely stunted and 258 ( 240 - 274 ) million were mildly , moderately or severely underweight in the developing countries .', 'in iran a study among 752 high school girls in sistan and baluchestan showed prevalence of 16.2% , 8.6% and 1.5% , for underweight , overweight and obesity , respectively .', 'the prevalence of malnutrition among elementary school aged children in tehran varied from 6% to 16% .']\n",
            "['background : the present study was carried out to assess the effects of community nutrition intervention based on advocacy approach on malnutrition status among school - aged children in shiraz , iran.materials and methods : this case - control nutritional intervention has been done between 2008 and 2009 on 2897 primary and secondary school boys and girls ( 7 - 13 years old ) based on advocacy approach in shiraz , iran .', 'the project provided nutritious snacks in public schools over a 2-year period along with advocacy oriented actions in order to implement and promote nutritional intervention . for evaluation of effectiveness of the intervention growth monitoring indices of pre- and post - intervention were statistically compared.results:the frequency of subjects with body mass index lower than 5% decreased significantly after intervention among girls ( p = 0.02 ) .', 'however , there were no significant changes among boys or total population .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have your own data, process them into the same structure then put them into the data/ folder"
      ],
      "metadata": {
        "id": "xZyXFXMNHUn2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next thing we need to do is to create high-ROUGE episodes for the training set, as introduced in the paper: https://aclanthology.org/2022.acl-long.450/,\n",
        "and the github introduction: https://github.com/nianlonggu/MemSum#addition-info-code-for-obtaining-the-greedy-summary-of-a-document-and-creating-high-rouge-episodes-for-training-the-model"
      ],
      "metadata": {
        "id": "AXhmJRo0Hh0Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.data_preprocessing.MemSum.utils import greedy_extract\n",
        "import json\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "ThmF2tO8EWcQ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_corpus = [ json.loads(line) for line in open(\"data/custom_data/train_CUSTOM_raw.jsonl\") ]\n",
        "for data in tqdm(train_corpus):\n",
        "    high_rouge_episodes = greedy_extract( data[\"text\"], data[\"summary\"], beamsearch_size = 2 )\n",
        "    indices_list = []\n",
        "    score_list  = []\n",
        "\n",
        "    for indices, score in high_rouge_episodes:\n",
        "        indices_list.append( indices )\n",
        "        score_list.append(score)\n",
        "\n",
        "    data[\"indices\"] = indices_list\n",
        "    data[\"score\"] = score_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E8lqASfWFTWj",
        "outputId": "42cd2bc8-8f23-4f41-dcd2-bcde31583b25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [01:44<00:00,  1.05s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have obtained the labels for the training set. This can be parallized if you have large training set.\n",
        "\n",
        "We can save the labeled training set to a new file:"
      ],
      "metadata": {
        "id": "Cn5dHZHMJ4Cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"data/custom_data/train_CUSTOM_labelled.jsonl\",\"w\") as f:\n",
        "    for data in train_corpus:\n",
        "        f.write(json.dumps(data) + \"\\n\")"
      ],
      "metadata": {
        "id": "Q4Ow4PMDJIAO"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! We are about to train MemSum!"
      ],
      "metadata": {
        "id": "fKtmefvVK2FE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "InGexStDK6v4"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "Ojiaw0SSK7vj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download pretrained word embedding\n",
        "\n",
        "MemSUM used the glove embedding (200dim), with three addition token embeddings for bos eos pad, etc.\n",
        "\n",
        "You can download the word embedding (a folder named glove/) used in this work:\n",
        "\n",
        "https://drive.google.com/drive/folders/1lrwYrrM3h0-9fwWCOmpRkydvmF6hmvmW?usp=sharing\n",
        "\n",
        "and put the folder under the model/ folder. \n",
        "\n",
        "Or you can do it using the code below:\n",
        "\n",
        "Make sure the structure looks like:\n",
        "\n",
        "1. MemSum/model/glove/unigram_embeddings_200dim.pkl\n",
        "2. MemSum/model/glove/vocabulary_200dim.pkl\n",
        "\n",
        "\n",
        "If not, you can change manually\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "NW2PUEZJK-rT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdown -q\n",
        "try:\n",
        "    os.system(\"rm -r model\")\n",
        "    os.makedirs(\"model/\")\n",
        "except:\n",
        "    pass\n",
        "!cd model/; gdown --folder https://drive.google.com/drive/folders/1lrwYrrM3h0-9fwWCOmpRkydvmF6hmvmW\n",
        "\n",
        "\n",
        "if not os.path.exists(\"model/glove\"):\n",
        "    try:\n",
        "        os.makedirs(\"model/glove\")\n",
        "        os.system(\"mv model/*.pkl model/glove/\")\n",
        "    except:\n",
        "        pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJuK4zGCK8sC",
        "outputId": "a0fb0b66-d89b-42bd-b4ed-8b378f799af3"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Retrieving folder list\n",
            "Processing file 1SVTHcgWJDvoVCsLfdvkaw5ICkihjUoaH unigram_embeddings_200dim.pkl\n",
            "Processing file 1SuF4HSe0-IBKWGtc1xqlzMHNDneiLi4- vocabulary_200dim.pkl\n",
            "Retrieving folder list completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SVTHcgWJDvoVCsLfdvkaw5ICkihjUoaH\n",
            "To: /content/MemSum/model/unigram_embeddings_200dim.pkl\n",
            "100% 320M/320M [00:01<00:00, 210MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1SuF4HSe0-IBKWGtc1xqlzMHNDneiLi4-\n",
            "To: /content/MemSum/model/vocabulary_200dim.pkl\n",
            "100% 4.16M/4.16M [00:00<00:00, 264MB/s]\n",
            "Download completed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Start training"
      ],
      "metadata": {
        "id": "Hfw7L6KRNKAV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note:\n",
        "1. you need to switch to the folder src/MemSum_Full;\n",
        "2. You can specify the path to training and validation set, the model_folder (where you want to store model checkpoints) and the log_folder (where you want to store the log info), and other parameters. \n",
        "3. You can provide the absolute path, or relative path, as shown in the example code below.\n",
        "4. n_device means the number of available GPUs"
      ],
      "metadata": {
        "id": "W50ktMCZNyz9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!cd src/MemSum_Full; python train.py -training_corpus_file_name ../../data/custom_data/train_CUSTOM_labelled.jsonl -validation_corpus_file_name ../../data/custom_data/val_CUSTOM_raw.jsonl -model_folder ../../model/MemSum_Full/custom_data/200dim/run0/ -log_folder ../../log/MemSum_Full/custom_data/200dim/run0/ -vocabulary_file_name ../../model/glove/vocabulary_200dim.pkl -pretrained_unigram_embeddings_file_name ../../model/glove/unigram_embeddings_200dim.pkl -max_seq_len 100 -max_doc_len 500 -num_of_epochs 10 -save_every 1000 -n_device 1 -batch_size_per_device 4 -max_extracted_sentences_per_document 7 -moving_average_decay 0.999 -p_stop_thres 0.6"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wPnWLxpkMYLy",
        "outputId": "51196d34-fc8b-4dfe-aa40-0e5da4891691"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0it [00:00, ?it/s]\r100it [00:00, 10630.33it/s]\n",
            "\r0it [00:00, ?it/s]\r100it [00:00, 13147.05it/s]\n",
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:566: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "0it [00:00, ?it/s]train.py:227: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  remaining_mask_np = np.ones_like( doc_mask_np ).astype( np.bool ) | doc_mask_np\n",
            "train.py:228: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  extraction_mask_np = np.zeros_like( doc_mask_np ).astype( np.bool ) | doc_mask_np\n",
            "24it [00:34,  1.38s/it]Starting validation ...\n",
            "train.py:308: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  remaining_mask_np = np.ones_like( doc_mask ).astype( np.bool ) | doc_mask\n",
            "train.py:309: DeprecationWarning: `np.bool` is a deprecated alias for the builtin `bool`. To silence this warning, use `bool` by itself. Doing this will not modify any behavior and is safe. If you specifically wanted the numpy scalar type, use `np.bool_` here.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  extraction_mask_np = np.zeros_like( doc_mask ).astype( np.bool ) | doc_mask\n",
            "val: 0.3103, 0.0874, 0.2774\n",
            "25it [00:48,  1.95s/it]\n",
            "24it [00:37,  1.63s/it]Starting validation ...\n",
            "val: 0.3175, 0.0926, 0.2828\n",
            "25it [00:53,  2.12s/it]\n",
            "24it [00:37,  1.54s/it]Starting validation ...\n",
            "val: 0.3223, 0.0957, 0.2860\n",
            "25it [00:53,  2.13s/it]\n",
            "24it [00:38,  1.62s/it][current_batch: 00100] loss: 0.485, learning rate: 0.000100\n",
            "Starting validation ...\n",
            "val: 0.3287, 0.1040, 0.2935\n",
            "25it [00:54,  2.17s/it]\n",
            "24it [00:38,  1.57s/it]Starting validation ...\n",
            "val: 0.3359, 0.1081, 0.3000\n",
            "25it [00:53,  2.13s/it]\n",
            "24it [00:38,  1.62s/it]Starting validation ...\n",
            "val: 0.3427, 0.1117, 0.3064\n",
            "25it [00:53,  2.16s/it]\n",
            "24it [00:38,  1.60s/it]Starting validation ...\n",
            "val: 0.3472, 0.1140, 0.3107\n",
            "25it [00:54,  2.17s/it]\n",
            "24it [00:38,  1.61s/it][current_batch: 00200] loss: 0.469, learning rate: 0.000100\n",
            "Starting validation ...\n",
            "val: 0.3524, 0.1136, 0.3137\n",
            "25it [00:55,  2.20s/it]\n",
            "24it [00:38,  1.61s/it]Starting validation ...\n",
            "val: 0.3603, 0.1184, 0.3216\n",
            "25it [00:53,  2.15s/it]\n",
            "24it [00:39,  1.66s/it]Starting validation ...\n",
            "val: 0.3635, 0.1206, 0.3258\n",
            "25it [00:54,  2.19s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testing trained model on custom dataset"
      ],
      "metadata": {
        "id": "47uWDs5nSZqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from summarizers import MemSum\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "import json\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5ziC45_cSfu9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_cal = rouge_scorer.RougeScorer(['rouge1','rouge2', 'rougeLsum'], use_stemmer=True)\n",
        "\n",
        "memsum_custom_data = MemSum(  \"model/MemSum_Full/custom_data/200dim/run0/model_batch_250.pt\", \n",
        "                  \"model/glove/vocabulary_200dim.pkl\", \n",
        "                  gpu = 0 ,  max_doc_len = 500  )"
      ],
      "metadata": {
        "id": "bcAh047kSjLj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_corpus_custom_data = [ json.loads(line) for line in open(\"data/custom_data/test_CUSTOM_raw.jsonl\")]"
      ],
      "metadata": {
        "id": "FJODv4CcUVwb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate( model, corpus, p_stop, max_extracted_sentences, rouge_cal ):\n",
        "    scores = []\n",
        "    for data in tqdm(corpus):\n",
        "        gold_summary = data[\"summary\"]\n",
        "        extracted_summary = model.extract( [data[\"text\"]], p_stop_thres = p_stop, max_extracted_sentences_per_document = max_extracted_sentences )[0]\n",
        "        \n",
        "        score = rouge_cal.score( \"\\n\".join( gold_summary ), \"\\n\".join(extracted_summary)  )\n",
        "        scores.append( [score[\"rouge1\"].fmeasure, score[\"rouge2\"].fmeasure, score[\"rougeLsum\"].fmeasure ] )\n",
        "    \n",
        "    return np.asarray(scores).mean(axis = 0)"
      ],
      "metadata": {
        "id": "c6f3Up_1UiWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluate( memsum_custom_data, test_corpus_custom_data, 0.6, 7, rouge_cal )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JaUWUcW0Ul9t",
        "outputId": "6c435b2f-11c5-4f65-e135-ad145058658b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 100/100 [00:16<00:00,  5.90it/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.37957819, 0.13561023, 0.3435555 ])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H86zC0sMWVhF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "To cite MemSum, please use the following bibtex:\n",
        "\n",
        "```\n",
        "@inproceedings{gu-etal-2022-memsum,\n",
        "    title = \"{M}em{S}um: Extractive Summarization of Long Documents Using Multi-Step Episodic {M}arkov Decision Processes\",\n",
        "    author = \"Gu, Nianlong  and\n",
        "      Ash, Elliott  and\n",
        "      Hahnloser, Richard\",\n",
        "    booktitle = \"Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)\",\n",
        "    month = may,\n",
        "    year = \"2022\",\n",
        "    address = \"Dublin, Ireland\",\n",
        "    publisher = \"Association for Computational Linguistics\",\n",
        "    url = \"https://aclanthology.org/2022.acl-long.450\",\n",
        "    doi = \"10.18653/v1/2022.acl-long.450\",\n",
        "    pages = \"6507--6522\",\n",
        "    abstract = \"We introduce MemSum (Multi-step Episodic Markov decision process extractive SUMmarizer), a reinforcement-learning-based extractive summarizer enriched at each step with information on the current extraction history. When MemSum iteratively selects sentences into the summary, it considers a broad information set that would intuitively also be used by humans in this task: 1) the text content of the sentence, 2) the global text context of the rest of the document, and 3) the extraction history consisting of the set of sentences that have already been extracted. With a lightweight architecture, MemSum obtains state-of-the-art test-set performance (ROUGE) in summarizing long documents taken from PubMed, arXiv, and GovReport. Ablation studies demonstrate the importance of local, global, and history information. A human evaluation confirms the high quality and low redundancy of the generated summaries, stemming from MemSum{'}s awareness of extraction history.\",\n",
        "}\n",
        "```"
      ],
      "metadata": {
        "id": "n1ra5Yp2Vj2O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "chvAK3HPV6I3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}